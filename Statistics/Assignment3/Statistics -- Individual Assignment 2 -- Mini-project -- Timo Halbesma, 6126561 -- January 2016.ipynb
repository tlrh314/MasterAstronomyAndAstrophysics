{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Methods for the Physical Sciences (5214SMFA3Y)\n",
    "## Individual mini-Project -- Model fitting and hypothesis testing: the search for WIMPs\n",
    "### Timo Halbesma, 6126561, 2016/01/08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enable showing plots inside iPython notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "import numpy\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import pandas\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Current data from the Fermi gamma-ray observatory is providing hints of the existence of anomalous extended GeV gamma-ray emission at the centre of our galaxy. This GeV continuum emission is causing excitement in the astroparticle-physics community, because it may be associated with the decay of the hitherto-undetected weakly interacting massive particles (WIMPs), which are thought to make up dark matter.\n",
    "\n",
    "This mini-project is based on analysing simulated spectral data from a hypothetical future gamma-ray observatory, whose main objective is to search for and study the extended dark matter decay signature in the centres of nearby galaxies. The data consists of a list of energies of photons detected in an observation of a nearby galaxy. Your task is to convert this data into a gamma-ray spectrum (this is just a histogram of photon numbers versus energy in discrete energy bins), carry out some simple tests and determine the shape of the continuum (with errors on the model parameters) as well as search for and characterise spectral emission lines.\n",
    "\n",
    "\n",
    "### Objectives\n",
    "_Important note_: throughout all of the following you should assume that in addition to any astrophysical source photons with spectra described below, there is an additional instrumental background photon continuum which contributes a constant number of photons per GeV. For the dataset you are given, $C = 1.5$ photons/GeV.\n",
    "\n",
    "### Question 1\n",
    "- The simple alternative to dark matter decay is that the continuum is produced by the combined unresolved emission from a large number of gamma-ray pulsars in the centre of the target galaxy. Assume that the spectrum of gamma-ray pulsars is a simple power- law following the relation: $$ dN = N_0 \\left( \\frac{E}{E_0} \\right)^{-\\Gamma} dE $$ where in our formalism, $dN$ is the number of photons expected in the infinitesimal energy range $dE$, and $N_0$ is the normalisation of the spectrum (in photons/GeV) at a fixed photon energy $E_0$. $\\Gamma$ is the power-law index and is also known as the 'photon index'. Assuming that $\\Gamma = 2$, use a _non-parametric significance test_ to compare the shape of this pulsar spectrum with that of your data (note that you don't need to know $N_0$!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know of three different non-parametric significance tests: Spearman's $\\rho$, the Kolmogorov-Smirnov and looking at the mean of the data. We will look at the KS-test first, where we check the theoretical cdf against the cdf obtained from the data.\n",
    "\n",
    "In the words of Tom Riley:\n",
    ">I would think about the **theoretical** cumulative distribution model as:\n",
    "> $$ p(E) = \\left( \\int\\limits_{\\forall E} \\frac{dN}{dE} dE \\right)^{-1} \\times \\frac{dN}{dE} $$ $$ P (E < E') = \\int\\limits_{0}^{E'} p(E) dE $$\n",
    "> Then compare the empirical cumulative distribution to the theoretical distribution by computing the above integral for E^{\\prime} being the energies in your raw data. Then calculate the KS test statistic based on the set of differences for the set {E^{\\prime}}. Note that the lower limit is zero if there is no **known** selection bias against photons with energies less than the lowest energy in the sample. The limits on the integral should in general be dependent on the sensitivity domain of the instrument.\n",
    "\n",
    "> This is what Phil means in the PDF document by comparing to the **shape** of the spectrum. Notice that p(E) is not dependent on N_0 nor E_0. When you have a constant finite photon background, then\n",
    "> $$ P(E < E') = \\frac{E'}{E_{\\rm max}} + \\int\\limits_{0}^{E'} p(E) dE $$\n",
    "> Here E_max is the same as the upper limit on the integral for normalisation of the pulsar spectrum, and should probably be set to the maximum **observed** photon energy.\n",
    "\n",
    "Here\n",
    "\\begin{align}\n",
    "p(E) &= \\left( \\int\\limits_{\\forall E} \\frac{dN}{dE} dE \\right)^{-1} \\times \\frac{dN}{dE} = \\left( \\int\\limits_{\\forall E} N_0 \\left( \\frac{E}{E_0} \\right)^{-\\Gamma} dE \\right)^{-1} \\times N_0 \\left( \\frac{E}{E_0} \\right)^{-\\Gamma} \\\\\n",
    "&= \\left[ \\left(\\frac{E}{1-\\Gamma}\\right) N_0 \\left(\\frac{E}{E_0}\\right)^{-\\Gamma} \\right]^{-1} \\times N_0 \\left( \\frac{E}{E_0} \\right)^{-\\Gamma} = \\frac{1-\\Gamma}{E} \\\\\n",
    "\\\\\n",
    "P(E < E') &= \\frac{E'}{E_{\\rm max}} + \\int\\limits_{0}^{E'} p(E) dE = \\frac{E'}{E_{\\rm max}} + \\int\\limits_{0}^{E'} \\frac{1-\\Gamma}{E} dE \\\\\n",
    "&= \\frac{E'}{E_{\\rm max}} + \\left[ - \\log(E) \\right]_{0}^{E'}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_and_clean_dataset(filename=\"Halbesma_energies.txt\"):\n",
    "    ''' read data from file\n",
    "        clean is not needed; file only contains one column with .2f '''\n",
    "    energies = numpy.genfromtxt(filename, skiprows=0)\n",
    "    \n",
    "    return energies\n",
    "\n",
    "\n",
    "def non_parametric_significance_test(energies):\n",
    "    ''' Assume spectum of gamma-ray pulsars is simple power low (not dark matter decay)\n",
    "        dN = N_0 (E/E_0)**(-Gamma) dE\n",
    "            \n",
    "    Assume Gamma = 2, then perform non-parametric significance test to compare shape\n",
    "        of pulsar spectrum with that of the data\n",
    "    '''\n",
    "    \n",
    "    energies = numpy.sort(energies)\n",
    "    min_energy = numpy.min(energies)\n",
    "    max_energy = numpy.max(energies)\n",
    " \n",
    "    p = energies/max_energy + (-1.*numpy.log10(energies) + numpy.log10(min_energy))\n",
    "    \n",
    "    # pyplot.plot(p)\n",
    "    # pyplot.show()    \n",
    "    \n",
    "    # Perform Kolmogorov-Smirnov test\n",
    "    \n",
    "    cdf = energies/numpy.max(energies) - numpy.log10(energies)\n",
    "    \n",
    "    model = numpy.random.uniform(0, numpy.max(energies), len(energies))**-2\n",
    "        \n",
    "    (D_value, p_value) = scipy.stats.ks_2samp(energies, model)\n",
    "    # (D_value, p_value) = scipy.stats.kstest(energies, 'powerlaw', (2.0, 0.0, 1.0))\n",
    "    \n",
    "    print \"The (Scipy built-in) K-S test gives us a D-value of {0:.5f}\".format(D_value),\n",
    "    print \"corresponding to a p-value of {0:.5f}.\".format(p_value)\n",
    "    \n",
    "    certain = 1.0 - p_value\n",
    "\n",
    "    print \"This means that with {0:.5f} certainty we can say\".format(certain),\n",
    "    print \"that the two samples are drawn from a different distribution.\"\n",
    "    print\n",
    "    \n",
    "    # Calculate Spearman's rho correlation co-efficient\n",
    "    n = len(energies)\n",
    "    rho, pval = scipy.stats.spearmanr(energies, model)\n",
    "    t = rho * numpy.sqrt((n - 2) / (1 - rho**2))\n",
    "    print \"Spearman's Rho = {0}, p-value = {1}\\nt = {2}\".format(rho, pval, t)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    energies = parse_and_clean_dataset()\n",
    "    non_parametric_significance_test(energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Performing non-parametric significance tests **\n",
    "\n",
    "There are two non-parametric significance tests we would like to perform, the K-S test and Spearman's $\\rho$.\n",
    "\n",
    "The Kolmogorov-Smirnov is implemented in Scipy. From the documentation:\n",
    "> This is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution.\n",
    "\n",
    "> If the K-S statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples are the same.\n",
    "\n",
    "Here we have a very low $p$-value, which would mean that we have to reject $H_0$ that the samples are drawn from the same distribution, so this would imply that the assumed model is very poor to describe the data.\n",
    "\n",
    "The next non-parametric significance test we can perform is Spearman's Rho test. From the documentation:\n",
    ">Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship\n",
    "\n",
    "> The two-sided p-value for a hypothesis test whose null hypothesis is that two sets of data are uncorrelated, has same dimension as rho.\n",
    "\n",
    "Here we see a high low $\\rho$ in the order of zero. This means that both samples are uncorrelated. The corresponding $p$-value given is the $p$-value for the null hypothesis $H_0$ that if the two sets of data are uncorrelated the obtained $\\rho$ would have the same dimensions as found.\n",
    "\n",
    "The calculated $P(E < E')$ does not seem to make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "- Fit the observed spectrum with continuum models and find the best-fitting model. You should compare the following three possibilities:\n",
    "    - A simple power-law.\n",
    "    - A broken power-law:\n",
    "    $$ dN = N_0 \\left( \\frac{E}{E_0} \\right)^{-\\Gamma_1} dE \\quad \\rm{ for } E \\leq E_{\\rm bk} $$ $$ dN = N_0 \\left( \\frac{E_{\\rm bk}}{E_0} \\right)^{-\\Gamma_1} \\left( \\frac{E}{E_{\\rm bk}} \\right)^{-\\Gamma_2} dE \\quad \\rm{ for } E > E_{\\rm bk} $$ where $E_{\\rm bk}$ is the break energy and $\\Gamma_1$ and $\\Gamma_2$ denote respectively the photon indices below and above the break energy.\n",
    "    - An exponentially cut-off power-law:\n",
    "    $$ dN = N_0 \\left( \\frac{E}{E_0}\\right)^{- \\Gamma} \\exp\\left(-E/E_{\\rm cut}\\right) dE $$ where $E_{\\rm cut}$ is the cut-off energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inspect_data(energies):\n",
    "    ''' Inspect the data to see the effect of the chosen binsize.\n",
    "        NB: not correct for background, bins with <20 counts are not cut '''\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = pyplot.subplots(3, 2, figsize=(12, 18))\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6]\n",
    "    \n",
    "    # Use a different set of 'best'-practice number of bins (equally spaced binsizes)\n",
    "    # htt/s://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width\n",
    "    N = len(energies)\n",
    "    # Freedman–Diaconis rule\n",
    "    h = (2*numpy.subtract(*numpy.percentile(energies, [75, 25]))*N**(-1./3))\n",
    "    FD = numpy.ceil((numpy.max(energies) - numpy.min(energies)) / h)\n",
    "    sturges = numpy.ceil(numpy.log2(N)+1)  # assumes normal distribution\n",
    "    rice = numpy.ceil(2*N**(1./3))\n",
    "    \n",
    "    bins = [sturges, rice, FD, int(numpy.sqrt(N)), 500, 700]\n",
    "    labels = [r'$\\log_2 (N)+1$', r'$2*N^{1/3}$', r'FD', r'$\\sqrt{N}$', \"fixed\", \"fixed\"]\n",
    "    for i, (ax, nr_of_bins) in enumerate(zip(axes, bins)):\n",
    "        counts, edges = numpy.histogram(energies,\n",
    "            range=[numpy.min(energies), numpy.max(energies)],\n",
    "            bins=nr_of_bins, density=False)\n",
    "        \n",
    "        ax.plot(edges[1:], counts, lw=3, ls=\"steps-mid\", label=labels[i])\n",
    "        ax.axhline(20, c=\"r\", lw=2, ls=\"dashed\")\n",
    "    \n",
    "        ax.set_title(\"nr_of_bins = {0}\".format(nr_of_bins))\n",
    "        ax.set_ylabel(\"Counts\")\n",
    "        ax.set_xlabel(\"E (GeV)\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.legend()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Binning the data **\n",
    "\n",
    "The first step is to visually inspect the data, and try to determine an optimal number of bins. We set a hard limit on the minimum number of counts in each bin. If the number of counts is higher than twenty, then we can assume the error on the counts in the bins is normally distributed and we can use the $\\chi^2$ statistic later on for our fitting routines. The optimal number of bins is not trivial, but some formulae are available which indicate the optimal binsize, or the optimal number of bins as a function of the sample size. We use several to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    energies = parse_and_clean_dataset()\n",
    "    inspect_data(energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot the red dashed horizontal line indicates the twenty count cut-off. We observe that underbinning results in missing features in the low-energy part of the spectrum, whereas over-binning leads to cutting the tail of the distribution. This will be a problem later on, because we will fit three different models. Should we cut the data above the break- or cut-off energy we will effectively only fit one model (the simple power law). We really want to have the features in the low-energy part of the spectrum in addition to not-igonoring the tail of the distribution.\n",
    "\n",
    "The solution to this problem could be to use unequally spaced bins, which according to dr. Uttley is perfectly fine (\"There is no reason you should not be able to use unevenly spaced bins\"). This way we will get a roughly equal count rate in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_data_adaptive(energies, adaptive=True):\n",
    "    ''' bin data in unevenly spaced bins --> count rate is roughly equal over all x '''\n",
    "\n",
    "    # Fit routines should like smaller numbers better *_*\n",
    "    energies = energies / (numpy.mean(energies))\n",
    "\n",
    "    if adaptive:\n",
    "        # Bin data using 'adaptive' bin sizes\n",
    "        counts, edges = numpy.histogram(energies,\n",
    "            range=[numpy.min(energies), numpy.max(energies)],\n",
    "            bins=numpy.percentile(energies, range(0, 101, 2)), density=False)\n",
    "    else:\n",
    "        N = len(energies)\n",
    "        # Freedman–Diaconis rule\n",
    "        h = (2*numpy.subtract(*numpy.percentile(energies, [75, 25]))*N**(-1./3))\n",
    "        FD = numpy.ceil((numpy.max(energies) - numpy.min(energies)) / h)\n",
    "        counts, edges = numpy.histogram(energies,\n",
    "            range=[numpy.min(energies), numpy.max(energies)],\n",
    "            bins=FD, density=False)\n",
    "    \n",
    "    # Calculate the binsizes (which is different for each bin!)\n",
    "    binsizes = numpy.zeros(len(counts), dtype=numpy.float64)\n",
    "    for i in range(0, len(edges)-1):\n",
    "        binsizes[i] = edges[i+1] - edges[i]  # in GeV\n",
    "\n",
    "    # Cut the bins with less than 20 counts --> Poisson\n",
    "    # NB using adaptive method there should not be bins with less than 20 counts (e.g. in the tail)\n",
    "    counts = counts[counts > 20]\n",
    "    # Reduce length of edges and binsize too\n",
    "    edges = edges[0:len(counts) + 1]  # +1 because len(edges) = len(counts) +1\n",
    "    binsizes = binsizes[0:len(counts)]\n",
    "\n",
    "        \n",
    "    # Calculate intrinsic error of bin. \n",
    "    # NB this must be done before correcting for background!\n",
    "    # We assume --> err = sqrt(N) is normally distributed (since counts > 20)\n",
    "    sample_size = len(energies)\n",
    "    err = numpy.sqrt(counts) / (sample_size * binsizes)\n",
    "    \n",
    "    # Correct for background counts: C = 1.5 photons/GeV\n",
    "    counts = counts - (1.5 * binsizes)\n",
    "  \n",
    "    # Calculate density\n",
    "    dens = counts / (sample_size * binsizes)\n",
    "\n",
    "    return dens, edges, err\n",
    "\n",
    "    \n",
    "def plot_histogram(energies, adaptive=False):\n",
    "    ''' Display a plot of the pyplot binned data, and self-binned data '''\n",
    "    dens, edges, err = bin_data_adaptive(energies, adaptive=adaptive)\n",
    "    \n",
    "    f, (ax1, ax2) = pyplot.subplots(2, 1, figsize=(12, 18))\n",
    "    # NB not corrected for background photons, bins not cut when counts < 20!\n",
    "    # So this can be used as a sanity check because it is only the binned data!\n",
    "    ax1.hist(energies/numpy.mean(energies), bins=143, facecolor='b',\n",
    "             label=\"pyplot binned\", alpha=0.5, normed=True)\n",
    "\n",
    "    ax1.plot(edges[1:], dens, lw=2, c=\"r\",\n",
    "             ls=\"steps-mid\", label=\"adaptive binned\" if adaptive else \"numpy binned\")\n",
    "    ax1.set_ylabel(\"p(x)\")\n",
    "    ax1.set_xlabel(r'$\\frac{E}{<E>}$')\n",
    "    ax1.set_title(\"Histogram of raw data plus {0} binned data\".format(\"adaptive\" if adaptive else \"numpy\"))\n",
    "    ax1.legend(loc=1)\n",
    "   \n",
    "    # Riley-style plot of the binned data, but log-log plot\n",
    "    ax2.plot(edges[1:], dens, lw=3, c=\"black\",\n",
    "             ls=\"steps-mid\", label=\"adaptive binned\" if adaptive else \"numpy binned\")\n",
    "    ax2.set_ylabel(\"p(x)\")\n",
    "    ax2.set_xlabel(r'$\\frac{E}{<E>}$')\n",
    "    # ax2.set_title(\"Log-log plot of adaptive binned data\")\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.legend(loc=1)\n",
    "\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Riley was kind enough to provide solutions for the previous assignment, and this assignment is similar to the last question of said assignment: why not use Riley's code as a starting point? Or in some cases simply steal Riley's code :-)... I mean, it's the teacher's code so it must be correct and this must be the way to go, right? Also, it's there, this is not a programming course but a statistics course, and nobody likes to invent the wheel all over again. Credits for quite a lot of code used troughout this assignment are mostly due to Riley, but significant changes have been made.\n",
    "\n",
    "\n",
    "In the words of Riley: \n",
    "> Let's first load in the data, normalise the ~~count rate~~ **energy** values by the mean ~~count rate~~ **energy** (anticipating that it will be easier to optimize our fit with smaller x values), bin the ~~count rate~~ **energy** values into evenly-sized bins, and discard the bins in the tails of the distribution if they have < 20 counts. We can then calculate the error on the probability density of each bin based on the assumption that we can approximate the binning process as following a Poisson distribution (it actually follows a binomial distribution, since we are dealing with the probability that a given x-value is binned into the bin of interest). Then we can set error = $\\sqrt{n}$, where $n$ is the number of values in the bin. This makes sense if you just notice that the variance of a Poisson distribution is the expected ~~count rate~~ **energy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    plot_histogram(energies, adaptive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind you, the blue histogram is generated from the raw data. Here matplotlib bins the data in a fixed number of bins (not adaptive), it is not corrected for background photons and bins with counts < 20 are not ignored (which will be found in the tail of the distribution). So this can be used as a sanity check because it is only the binned data, nothing else happend. Since the background is relatively low (only 1.5 photon/GeV) one does not visually observe that difference. The second plot is generated using Riley's code and changing the axis to log-log. If we expect a power law, then the binned data should look like a straight line in a log-log plot where the slope is the power law index.\n",
    "\n",
    "Now also look at the non-adaptive histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    plot_histogram(energies, adaptive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is also for non-adpative binning, using the Freedman-Diaconis scheme for the number of bins. We can see clearly from the histogram that the data is cut from the tail (see the red line, it stops between 3 and 4 $E/<E>$) since the count rate falls below 20 counts.\n",
    "\n",
    "Next, we define our three models, our statistical model, write a fitting routine and output the MLEs and chisq/dof. The method used is copy-pasted from Riley's code but adjusted for the new models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our three models\n",
    "def simple_power_law(parm, edges):\n",
    "    N_0 = parm[0]\n",
    "    E_0 = parm[1]\n",
    "    Gamma = parm[2]\n",
    "    \n",
    "    # We have given the edges which allow us to do the integration over the bin\n",
    "    E_left = edges[:-1]\n",
    "    E_right = edges[1:]\n",
    "    binsize = E_right - E_left\n",
    "    \n",
    "    # Formula integrated over bin\n",
    "    counts = (N_0 * E_right/(1. - Gamma) * (E_right/E_0)**(-1. * Gamma)) \\\n",
    "        - (N_0 * E_left/(1. - Gamma) * (E_left/E_0)**(-1. * Gamma))\n",
    "    \n",
    "    # Now transform to density instead of counts\n",
    "    dens = counts / binsize\n",
    "    return dens\n",
    "\n",
    "\n",
    "def broken_power_law(parm, edges):\n",
    "    N_0 = parm[0]\n",
    "    E_0 = parm[1]\n",
    "    Gamma_1 = parm[2]\n",
    "    Gamma_2 = parm[3]\n",
    "    E_bk = parm[4]\n",
    "    \n",
    "    # We have given the edges which allow us to do the integration over the bin\n",
    "    E_left = edges[:-1]\n",
    "    E_right = edges[1:]\n",
    "    binsize = E_right - E_left\n",
    "    E_center = (E_left + E_right)/2.\n",
    "    \n",
    "    E_below_left = E_left[E_center <= E_bk]\n",
    "    E_below_right = E_right[E_center <= E_bk]\n",
    "    E_above_left = E_left[E_center > E_bk]\n",
    "    E_above_right = E_right[E_center > E_bk]\n",
    "    \n",
    "    # Formula integrated over bin\n",
    "    counts_below = \\\n",
    "        (N_0 * E_below_right/(1. - Gamma_1)\\\n",
    "        * (E_below_right/E_0)**(-1. * Gamma_1)) \\\n",
    "        - (N_0 * E_below_left/(1. - Gamma_1) \\\n",
    "        * (E_below_left/E_0)**(-1. * Gamma_1))\n",
    "        \n",
    "    counts_above = \\\n",
    "        (N_0 * (E_bk/E_0)**(-1. * Gamma_1) * E_above_right/(1. - Gamma_2)\\\n",
    "        * (E_above_right/E_bk)**(-1. * Gamma_2)) \\\n",
    "        - (N_0 * (E_bk/E_0)**(-1. * Gamma_1) * E_above_left/(1. - Gamma_2)\\\n",
    "        * (E_above_left/E_bk)**(-1. * Gamma_2))\n",
    "        \n",
    "    counts = numpy.concatenate([counts_below, counts_above])\n",
    "\n",
    "    # Now transform to density instead of counts\n",
    "    dens = counts / binsize\n",
    "    return dens\n",
    "\n",
    "\n",
    "def exponentially_cut_off_power_law(parm, edges):\n",
    "    N_0 = parm[0]\n",
    "    E_0 = parm[1]\n",
    "    Gamma = parm[2]\n",
    "    E_cut = parm[3]\n",
    "\n",
    "    # We have given the edges which allow us to do the integration over the bin\n",
    "    E_left = edges[:-1]\n",
    "    E_right = edges[1:]\n",
    "    binsize = E_right - E_left\n",
    "    \n",
    "    counts = (-1. * N_0 * (E_right/E_0)**(-1. * Gamma)\\\n",
    "        * E_right * scipy.special.expn(Gamma, E_right/E_cut)) \\\n",
    "        - (-1. * N_0 * (E_left/E_0)**(-1. * Gamma) \\\n",
    "        * E_right * scipy.special.expn(Gamma, E_left/E_cut))\n",
    "    \n",
    "    # Now transform to density instead of counts\n",
    "    dens = counts / binsize\n",
    "    return dens\n",
    "\n",
    "\n",
    "# Define the statistical model, in this case we shall use a chi-squared distribution, assuming normality in the errors\n",
    "def stat(parm, edges, y, dy, dist):\n",
    "    if \"gauss\" in dist:\n",
    "        dist = dist.split(\"_\")[0]\n",
    "        E_cent = parm[-1]\n",
    "        N_line = parm[-2]\n",
    "    if dist == \"simple\":\n",
    "        ymod = simple_power_law(parm, edges)\n",
    "    elif dist == \"broken\":\n",
    "        ymod = broken_power_law(parm, edges)\n",
    "    elif dist == \"expcut\":\n",
    "        ymod = exponentially_cut_off_power_law(parm, edges)\n",
    "    else:\n",
    "        print \"This function is not defined....choose another\"\n",
    "        return None \n",
    "    if \"gauss\" in dist:\n",
    "        ymod += gauss(parm, edges)\n",
    "    X = sum((y - ymod)**2 / dy**2)\n",
    "    return(X)\n",
    "\n",
    "\n",
    "def fit_observed_spectrum_with_continuum_models(energies, find_emissionline=False, adaptive=True):\n",
    "    # change nr_of_bins to an integer value to change the number of bins.\n",
    "    dens, edges, err = bin_data_adaptive(energies, adaptive=adaptive)\n",
    "        \n",
    "    # define an array of different dist names\n",
    "    dists = [\"simple\", \"broken\", \"expcut\"]\n",
    "    results = {}\n",
    "\n",
    "    for i in xrange(len(dists)):\n",
    "        # bounds=None\n",
    "        if dists[i] == \"simple\":\n",
    "            # [N_0, E_0, Gamma]\n",
    "            parm = [1.1, 1.5, 2.0]\n",
    "            bounds = [(None, None), (None, None), (None, None)]\n",
    "        elif dists[i] == \"broken\":\n",
    "            # [N_0, E_0, Gamma_1, Gamma_2, E_bk]\n",
    "            parm = [1.2, 1.3, 2.0, 2.0, 2.0]\n",
    "            bounds = [(None, None), (None, None), (None, None), (None, None), (numpy.min(edges), numpy.max(edges))]\n",
    "        elif dists[i] == \"expcut\":\n",
    "            # [N_0, E_0, Gamma, E_cut]\n",
    "            parm = [1.3, 2.2, 1.5, 1.0]\n",
    "            bounds = [(None, None), (None, None), (None, None), (numpy.min(edges), numpy.max(edges))]\n",
    "        if find_emissionline:\n",
    "            dists[i] += \"_gauss\"\n",
    "            # NB if the emission line will be fitted, this is where the initial guess on its parameters \n",
    "            # should be appended to the parm list, which contains the initial guess parameters.\n",
    "            parm.append(0.01)  # N_line guess\n",
    "            parm.append(1.2)  # E_cent guess\n",
    "            bounds.append((0.0, None))  # bound on L_line\n",
    "            bounds.append((1.0, 1.4))  # bound on E_cent\n",
    "        \n",
    "        result = scipy.optimize.minimize(stat, parm, args=(edges, dens, err, dists[i]),\n",
    "                                             method='L-BFGS-B', bounds=bounds)\n",
    "        results[dists[i]] = result\n",
    "    \n",
    "        ml_vals = result[\"x\"]\n",
    "        ml_func = result[\"fun\"]\n",
    "        dof = len(dens) - len(ml_vals)\n",
    "        print \"[MLEs], chisq/dof:\", ml_vals, ml_func/dof\n",
    "        ch = scipy.stats.chi2(dof)\n",
    "        pval = 1.0 - ch.cdf(ml_func)\n",
    "        print \"The p-value for the {0} distribution is: {1:.5f}\".format(dists[i], pval)\n",
    "    \n",
    "    plot_spectrum_with_continuum_models(energies, results, discrete=True, adaptive=adaptive)\n",
    "\n",
    "    return results\n",
    "\n",
    "    \n",
    "def plot_spectrum_with_continuum_models(energies, results, discrete=True, adaptive=True):\n",
    "    dens, edges, err = bin_data_adaptive(energies, adaptive=adaptive)\n",
    "    cols = {\"simple\": \"red\", \"broken\": \"green\", \"expcut\": \"blue\",\n",
    "            \"simple_gauss\": \"red\", \"broken_gauss\": \"green\", \"expcut_gauss\": \"blue\"}\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = pyplot.subplots(2, 2, figsize=(12, 9))\n",
    "    \n",
    "    # Plot the data\n",
    "    for dist, ax in zip(results.keys(), [ax1, ax2, ax3]):\n",
    "        ax.plot(edges[1:], dens, lw=1, color=\"black\",\n",
    "                linestyle=\"steps-mid\")\n",
    "        ax.set_ylabel(\"p(x)\")\n",
    "        ax.set_xlabel(r'$\\frac{E}{<E>}$')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        # ax.tick_params(labelsize=15)\n",
    "        # ax.legend()\n",
    "        ax.set_title(dist)    \n",
    "    \n",
    "    # Choose to display model in same bins as data (discrete) or continuously \n",
    "    if not discrete:\n",
    "        edges = numpy.arange(numpy.min(energies)/numpy.mean(energies),\n",
    "                             numpy.max(energies)/numpy.mean(energies), 0.001)\n",
    "    \n",
    "    for dist, result in results.iteritems():\n",
    "        ml_vals = result[\"x\"]\n",
    "        ml_func = result[\"fun\"]\n",
    "        if dist == \"simple\":\n",
    "            ax1.plot(edges[1:], simple_power_law(ml_vals, edges), c=cols.get(dist, \"k\"),\n",
    "                     label=dist,  drawstyle=\"steps-mid\")\n",
    "        if dist == \"simple_gauss\":\n",
    "            ax1.plot(edges[1:], simple_power_law(ml_vals, edges)+gauss(ml_vals, edges),\n",
    "                     c=cols.get(dist, \"k\"), label=dist,  drawstyle=\"steps-mid\")\n",
    "        elif dist == \"broken\":\n",
    "            ax2.axvline(ml_vals[4], ls=\"dashed\", c=cols.get(dist, \"k\"))\n",
    "            ax2.plot(edges[1:], broken_power_law(ml_vals, edges), c=cols.get(dist, \"k\"),\n",
    "                     label=dist, drawstyle=\"steps-mid\")\n",
    "        elif dist == \"broken_gauss\":\n",
    "            ax2.axvline(ml_vals[4], ls=\"dashed\", c=cols.get(dist, \"k\"))\n",
    "            ax2.plot(edges[1:], broken_power_law(ml_vals, edges)+gauss(ml_vals, edges),\n",
    "                     c=cols.get(dist, \"k\"), label=dist, drawstyle=\"steps-mid\")\n",
    "        elif dist == \"expcut\":\n",
    "            ax3.axvline(ml_vals[3], ls=\"dashed\", c=cols.get(dist, \"k\"))\n",
    "            ax3.plot(edges[1:], exponentially_cut_off_power_law(ml_vals, edges),\n",
    "                     label=dist, c=cols.get(dist, \"k\"), drawstyle=\"steps-mid\")\n",
    "        elif dist == \"expcut_gauss\":\n",
    "            ax3.axvline(ml_vals[3], ls=\"dashed\", c=cols.get(dist, \"k\"))\n",
    "            ax3.plot(edges[1:], exponentially_cut_off_power_law(ml_vals, edges)+gauss(ml_vals, edges),\n",
    "                     label=dist, c=cols.get(dist, \"k\"), drawstyle=\"steps-mid\")\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    fig.delaxes(ax4)\n",
    "    pyplot.draw()\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fit_results = fit_observed_spectrum_with_continuum_models(energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, in the words of Riley: \n",
    "> Now we run through a loop of these model functions and fit each one in turn to obtain the MLEs and p-values based on the best-fit model evaluation ($\\chi^2$) value. The results along with plots against the data are seen ~~below~~ ** above**. (...) Notice we have actually shown the binned model in the plot, since it represents the procedure best - we do not know how the data behaves throughout any given bin, and although we have somewhat taken this into account by defining our model values as the probability density distribution ~~averaged~~ **integrated** over the bin, showing the binned model values presents the results in the clearest way. \n",
    "\n",
    "We are also interested in seeing the plot of the non-binned model line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_spectrum_with_continuum_models(energies, fit_results, discrete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above plots we have indicated the break- or cutoff energy as a vertical, dashed line. We have also examined the effect of the number of bins (equally spaced bins) on the $p$-value. We do observe changes in the obtained $p$-values for the fit depending on the number of bins. In some cases the chosen number of bins results in a found break- or cutoff energy that lies outside the power law because this results in less than 20 counts in the tail, meaning we could cut the energies above break- or cutoff energy. This is probably not a good fit as it is equal to the simple power law (in the case of the broken power law), and similar (in the case of the exponentially cut-off power law). However, it is not good practice to use the number of bins as a fit value, (i.e. optimising the number of bins to obtained the highest $p$-value). But the problem is that if we adaptive binsizes we get few datapoints in the tail such that the fit of the tail makes little sense (too few dataponts, large binsizes such that it matters a lot which energy value is chosen to represent the bin), but sufficiently high resolution in the low-energy part of the spectrum such that we expect to be able to resolve emission lines, should there be any.\n",
    "\n",
    "To try and tackle the big-bin-tail-problem we have changed the formulae which were initially the analytical formulae used with the average value of the bin, to integrated analytical formulae given in the assignment. This way the energy valye is actually integrated over the bin and we expect the bin to be better represented in the fitting routine. As a result, the function implementations do not really look elegant anymore, but the result should be better than fitting using the averaged bin value.\n",
    "\n",
    "From the graph where we do not use the binned data to plot the model but we use a more continuously array instead it can be clearly seen that there are deviations of the model from the data, and that changing the energy value to represent the bin matters.\n",
    "\n",
    "The next step would be to obtain confidence intervals on the obtained MLE's. In this case, however, we notice that the $p$-values are so low that calculating confidence intervals would make little to no sense whatoever. Nontheless, we were... inspired... again by Riley's method to use the `scipy.optimize.curve_fit` routine to calculate confidence intervals and to plot the residuals. In the words of Riley:\n",
    "> (...) although this routine does not return the actual value of the statistic, it does return the Hessian, and we can simply obtain the statistic ourselves afterwards. This is shown below. It is worth noting that the p-value would change quite a bit if we were to change the number of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simple_power_law_wrapper(edges, parm0, parm1, parm2):\n",
    "    \"\"\" Same same, but scipy.optimize.minimize expects different form of\n",
    "        function than scipy.optimize.curve_fit does. So use this wrapper :-)... \"\"\"\n",
    "    N_0 = parm0\n",
    "    E_0 = parm1\n",
    "    Gamma = parm2\n",
    "\n",
    "    return simple_power_law((parm0, parm1, parm2), edges)\n",
    "\n",
    "def broken_power_law_wrapper(edges, parm0, parm1, parm2, parm3, parm4):\n",
    "    N_0 = parm0\n",
    "    E_0 = parm1\n",
    "    Gamma_1 = parm2\n",
    "    Gamma_2 = parm3\n",
    "    E_bk = parm4\n",
    "    \n",
    "    return broken_power_law((parm0, parm1, parm2, parm3, parm4), edges)\n",
    "\n",
    "\n",
    "def exponentially_cut_off_power_law_wrapper(edges, parm0, parm1, parm2, parm3):\n",
    "    N_0 = parm0\n",
    "    E_0 = parm1\n",
    "    Gamma = parm2\n",
    "    E_cut = parm3\n",
    "\n",
    "    return exponentially_cut_off_power_law((parm0, parm1, parm2, parm3), edges)\n",
    "\n",
    "\n",
    "def print_confidence_intervals_and_plot_residuals(model, result,\n",
    "        adaptive=True, find_emissionline=False, print_cis=True,\n",
    "        plot_residuals=True, discrete=True):\n",
    "    dens, edges, err = bin_data_adaptive(energies, adaptive=adaptive)\n",
    "\n",
    "    # Conifdence intervals\n",
    "    functionpicker = {\"simple\": simple_power_law_wrapper ,\n",
    "                      \"broken\": broken_power_law_wrapper,\n",
    "                      \"expcut\": exponentially_cut_off_power_law_wrapper}\n",
    "    function = functionpicker.get(model, None)\n",
    "    \n",
    "    if find_emissionline:\n",
    "        function = gauss_wrapper\n",
    "    \n",
    "    if not function:\n",
    "        print \"Error: incorrect model name used\"\n",
    "        return\n",
    "    \n",
    "    ml_vals = result[\"x\"]\n",
    "    moddof = len(ml_vals)\n",
    "    \n",
    "    if print_cis:\n",
    "        ml_func = result[\"fun\"]\n",
    "        dof = len(dens) - moddof\n",
    "        \n",
    "        print \"Results for the '{0}' model:\".format(model)\n",
    "        print \"  Using scipy.optimize.minimze to minimize chi^2 yields:\"\n",
    "        print \"    [MLEs], chisq/dof:\", ml_vals, ml_func/dof\n",
    "        print\n",
    "    \n",
    "    ml_vals, ml_covar = scipy.optimize.curve_fit(function, edges, dens, p0=ml_vals, sigma=err,)\n",
    "    ml_funcval = stat(ml_vals, edges, dens, err, model)\n",
    "    \n",
    "    if not result[\"success\"]:\n",
    "        print \"  scipy.optimize.curve_fit broke down!\\n    Reason: '{0}'\".format(result[\"message\"])\n",
    "        print \"  No confidence intervals have been calculated.\"\n",
    "        return\n",
    "\n",
    "    dof = len(dens) - moddof\n",
    "    errs = numpy.sqrt(numpy.diag(ml_covar))\n",
    "    if print_cis:\n",
    "        print \"  Using scipy.optimize.curve_fit to obtain confidence intervals yields:\"\n",
    "        print \"    N_0 = {0:.3f} +/- {1:.3f}\".format(ml_vals[0], err[0])\n",
    "        print \"    E_0 = {0:.3f} +/- {1:.3f}\".format(ml_vals[1], err[1])\n",
    "        print \"    {parm2_name} = {0:.3f} +/- {1:.3f}\".format(ml_vals[2], err[2],\n",
    "            parm2_name=\"Gamma\" if model in [\"simple\", \"simple_gauss\", \"expcut\", \"expcut_gauss\"] else \"Gamma_1\")\n",
    "        if moddof > 3 and not find_emissionline:\n",
    "            print \"    {parm3_name} = {0:.3f} +/- {1:.3f}\".format(ml_vals[3], err[3],\n",
    "                parm3_name=\"Gamma_2\" if model in [\"broken\", \"broken_gauss\"] else \"E_cut\")\n",
    "        if moddof > 4 and not find_emissionline:\n",
    "            print \"    E_bk = {0:.3f} +/- {1:.3f}\".format(ml_vals[4], err[4])\n",
    "        if find_emissionline:\n",
    "            if moddof == 7:  # broken\n",
    "                print \"    Gamma_2 = {0:.3f} +/- {1:.3f}\".format(ml_vals[3], err[3])\n",
    "                print \"    E_bk = {0:.3f} +/- {1:.3f}\".format(ml_vals[4], err[4])\n",
    "                print \"    N_line = {0:.3f} +/- {1:.3f}\".format(ml_vals[5], err[5])\n",
    "                print \"    E_cent = {0:.3f} +/- {1:.3f}\".format(ml_vals[6], err[6])\n",
    "            if moddof == 6:  # expcut\n",
    "                print \"    E_bk = {0:.3f} +/- {1:.3f}\".format(ml_vals[3], err[3])\n",
    "                print \"    N_line = {0:.3f} +/- {1:.3f}\".format(ml_vals[4], err[4])\n",
    "                print \"    E_cent = {0:.3f} +/- {1:.3f}\".format(ml_vals[5], err[5])\n",
    "            if moddof == 5:  # simple\n",
    "                print \"    N_line = {0:.3f} +/- {1:.3f}\".format(ml_vals[3], err[3])\n",
    "                print \"    E_cent = {0:.3f} +/- {1:.3f}\".format(ml_vals[4], err[4])\n",
    "        \n",
    "        ch = scipy.stats.chi2(dof)\n",
    "        pval = 1.0 - ch.cdf(ml_funcval)\n",
    "        print \"    p-value for this fit = {0:.5f}\".format(pval)\n",
    "        \n",
    "    # Residuals plot\n",
    "    functionpicker = {\"simple\": simple_power_law,\n",
    "                      \"broken\": broken_power_law,\n",
    "                      \"expcut\": exponentially_cut_off_power_law}\n",
    "    function = functionpicker.get(model, None)\n",
    "\n",
    "    if find_emissionline:\n",
    "        function = True\n",
    "    \n",
    "    if not function:\n",
    "        print \"Error: incorrect model name used\"\n",
    "        return\n",
    "    \n",
    "    if plot_residuals:\n",
    "        binsize = numpy.array([edges[i+1] - edges[i] for i in range(len(dens))])\n",
    "        \n",
    "        colourpicker = {\"simple\": \"red\", \"broken\": \"green\", \"expcut\": \"blue\",\n",
    "                        \"simple_gauss\": \"red\", \"broken_gauss\": \"green\", \"expcut_gauss\": \"blue\"}\n",
    "        col = colourpicker.get(model, \"black\")\n",
    "        \n",
    "        fig = pyplot.subplots(2, 1, figsize=(12, 9))\n",
    "        gs1 = matplotlib.gridspec.GridSpec(3, 3)\n",
    "        gs1.update(hspace=0)\n",
    "        ax1 = pyplot.subplot(gs1[:-1,:])\n",
    "        ax2 = pyplot.subplot(gs1[-1,:])\n",
    "        # ax1.tick_params(labelsize=15)\n",
    "        # ax2.tick_params(labelsize=15)\n",
    "        # ax1.tick_params(labelbottom='off')\n",
    "\n",
    "        ax1.set_xlim(0.2, 5.1)\n",
    "        ax2.set_xlim(0.2, 5.1)\n",
    "\n",
    "        ax1.set_ylabel(\"p(x)\")\n",
    "        ax2.set_ylabel(\"residuals\")\n",
    "        ax2.set_xlabel(r'$\\frac{E}{<E>}$')\n",
    "        ax1.errorbar(edges[1:] + binsize/2., dens, xerr=binsize/2., yerr=err,\n",
    "                     label=\"data\", c=\"black\", marker='o', linestyle='', markersize=3)\n",
    "        \n",
    "        ax2.axhline(y=0, linewidth=2, linestyle='dashed', c=\"black\")\n",
    "\n",
    "        if not find_emissionline:\n",
    "            ax2.errorbar(edges[1:] + binsize/2., dens - function(ml_vals, edges), yerr=err,\n",
    "                         c=\"black\", drawstyle=\"steps-mid\")\n",
    "        else:\n",
    "            ax2.errorbar(edges[1:] + binsize/2., dens - gauss_plot(ml_vals, edges, model), yerr=err,\n",
    "                         c=\"black\", drawstyle=\"steps-mid\")\n",
    "        \n",
    "        # Choose to display model in same bins as data (discrete) or continuously \n",
    "        if not discrete:\n",
    "            edges = numpy.arange(numpy.min(energies)/numpy.mean(energies),\n",
    "                                 numpy.max(energies)/numpy.mean(energies), 0.001)\n",
    "            binsize = 0.001\n",
    "        if not find_emissionline:\n",
    "            ax1.plot(edges[1:] + binsize/2., function(ml_vals, edges),\n",
    "                    label=model, c=col, drawstyle='steps-mid')\n",
    "        else:\n",
    "            ax1.plot(edges[1:] + binsize/2., gauss_plot(ml_vals, edges, model),\n",
    "                    label=model, c=col, drawstyle='steps-mid')            \n",
    "        \n",
    "        if model == \"broken\":\n",
    "            break_or_cut = ml_vals[4]\n",
    "            ax1.axvline(break_or_cut, ls=\"dashed\", c=col)\n",
    "        elif model == \"expcut\":\n",
    "            break_or_cut = ml_vals[3]\n",
    "            ax1.axvline(break_or_cut, ls=\"dashed\", c=col)\n",
    "\n",
    "        ax1.legend()\n",
    "        \n",
    "        # fix overlapping ticks\n",
    "        ax2.set_ylim(ymax=0.099)\n",
    "        ax1.tick_params(labelbottom='off')\n",
    "        nbins = len(ax2.get_yticklabels())\n",
    "        ax2.yaxis.set_major_locator(MaxNLocator(nbins=nbins, prune='upper'))\n",
    "        \n",
    "        # log-log\n",
    "        ax1.set_xscale('log')\n",
    "        ax2.set_xscale('log')\n",
    "        ax1.set_yscale('log')\n",
    "        \n",
    "        pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for model, result in fit_results.iteritems():\n",
    "        print_confidence_intervals_and_plot_residuals(model, result)\n",
    "        print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the words of Riley:\n",
    "> The plot above shows the results of the optimization.\n",
    "\n",
    "Firstly, we observe that the $p$-values of the fits are rather low, which indicates that the fits should probably be rejected. However, we are trying to fit an emission line next hopefully improving the obtained $p$-value. In order to find our initial expected value for the emission line energy and its intensity we have to inspect the residuals. We do not really see hints for emission lines in the residuals, but maybe there is a little something around 1.2 $E/<E>$, so we will try to fit this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "- Search for emission lines, which may carry crucial information if they are associated with specific dark matter particle decays. You may assume a Gaussian profile to fit the emission line:\n",
    "$$ dN = \\frac{N_{\\rm line}}{\\sigma \\sqrt{2 \\pi}} \\exp\\left( \\frac{(E - E_{\\rm cent})^2)}{2 \\sigma^2} \\right) dE $$ where $E_{\\rm cent}$ is the line centroid energy, $\\sigma$ is the line width and $N_{\\rm line}$ is the expected total number of photons contained in the line. The width of any lines is set by the instrumental resolution (which is a function of energy) and is given by:\n",
    "\n",
    "$$ \\sigma = 2.0 \\sqrt{\\frac{E_{\\rm cent}}{200 \\, \\rm{ GeV}}} \\, \\rm{ Gev} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    "\n",
    "\n",
    "So the first thing we will do is define the Gauss function, which is integrated since we integrate over the bins. A bit of caution is required here since sigma is given in units of GeV, but we have chosen at the start to divide the mean of the energies out. So we need to correct sigma for this such that it is unitless. Mind you, E_cent is also given in $E/<E>$! So therefore we are left with multiplying the denominator in the sqrt with the mean. Next, we integrate over the bins such that the indices of the gauss function match with the indices of the power-law functions so we can simply take the sum of both functions. For this purpose we have to define a wrapper around the gaus function to ensure all fit-parameters are passed to the correct function. This is not trivial, but since all three model power-laws have different numbers of fit parameters this is very well manageable. Note that the parameters to pass to the Gauss function are appended to the list of guess (initial) parameters, so they are always the last two in the tuple. \n",
    "\n",
    "Mind you, the code for stat (the ($chi^2$ model) has also been updated because the scipy.optimize.minimize and the scipy.optimize.curve_fit functions eat the fit parameters in a different representation (tuple vs explicitly passing them). Finally, to generate the residuals the function definitions including the emission line has to be used. For this purpose there is a gauss_plot wrapper function to return the continuum model plus emission line function values.\n",
    "\n",
    "For the sake of checking the Gauss function a small plot is provided to ensure that the emission line looks sensible and behaves as expected (which it does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gauss(parm, edges):\n",
    "    E_cent = parm[-1]\n",
    "    N_line = parm[-2]\n",
    "    mean = numpy.mean(parse_and_clean_dataset()) \n",
    "    sigma = 2.0 * numpy.sqrt(E_cent / (200.*mean))  # Because E_cent given is E_cent/<E>\n",
    "    # print sigma\n",
    "    \n",
    "    # We have given the edges which allow us to do the integration over the bin\n",
    "    E_left = edges[:-1]\n",
    "    E_right = edges[1:]\n",
    "    binsize = E_right - E_left\n",
    "    \n",
    "    # Formula integrated over bin\n",
    "    cst = N_line/(sigma*numpy.sqrt(2*numpy.pi)) * numpy.sqrt(numpy.pi/2) * -1.0*sigma\n",
    "    counts = cst*scipy.special.erf((E_cent-E_right)/(numpy.sqrt(2)*sigma)) - \\\n",
    "        cst*scipy.special.erf((E_cent-E_left)/(numpy.sqrt(2)*sigma))\n",
    "    \n",
    "    # Now transform to density instead of counts\n",
    "    dens = counts / binsize\n",
    "    \n",
    "    return dens\n",
    "\n",
    "def gauss_wrapper(edges, parm0, parm1, parm2, parm3, parm4, parm5=None, parm6=None):\n",
    "    ''' The different models have different nr of fit parm, but always the last two parameters \n",
    "        are the Gauss parameters.  '''\n",
    "    # simple has three fit parameters\n",
    "    parm_pl = (parm0, parm1, parm2)\n",
    "    parm_gauss = (parm3, parm4)\n",
    "    function = simple_power_law\n",
    "    if parm5:\n",
    "        # expcut has four fit parameters\n",
    "        # not simple but expcut if parm6 is not also given\n",
    "        parm_pl = (parm0, parm1, parm2, parm3)\n",
    "        parm_gauss = (parm4, parm5)\n",
    "        function = exponentially_cut_off_power_law\n",
    "    if parm6:\n",
    "        # broken has five fit parameters\n",
    "        # not expcut but broken\n",
    "        parm_pl = (parm0, parm1, parm2, parm3, parm4)\n",
    "        parm_gauss = (parm5, parm6)\n",
    "        function = broken_power_law\n",
    "        \n",
    "    return function(parm_pl, edges) + gauss(parm_gauss, edges)\n",
    "\n",
    "def gauss_plot(parm, edges, dist):\n",
    "    if \"gauss\" in dist:\n",
    "        dist = dist.split(\"_\")[0]\n",
    "        E_cent = parm[-1]\n",
    "        N_line = parm[-2]\n",
    "    if dist == \"simple\":\n",
    "        ymod = simple_power_law(parm, edges)\n",
    "    elif dist == \"broken\":\n",
    "        ymod = broken_power_law(parm, edges)\n",
    "    elif dist == \"expcut\":\n",
    "        ymod = exponentially_cut_off_power_law(parm, edges)\n",
    "    else:\n",
    "        print \"This function is not defined....choose another\"\n",
    "        return None \n",
    "    if \"gauss\" in dist:\n",
    "        ymod += gauss(parm, edges)\n",
    "    return ymod\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = numpy.arange(0, 200/28, 0.01)\n",
    "    pyplot.plot(x[1:], gauss((0.001, 2), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check we first look at the behaviour of the Gauss fuction. parm=(N_line, E_cent), so if we specify a given E_cent, we should see a peak centered around that value. We can use N_line to set the intensity of the emission line. Note that sigma is not self-given as it is set by the instrumental resolution. It is a function of E_cent, and indeed we see that it changes for different E_cent given. Now we can simply fit using the sum of the gauss function and the powerlaw model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    fit_results = fit_observed_spectrum_with_continuum_models(energies, adaptive=True, find_emissionline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the expcut_gauss has weird MLE's and therefore the plot does not show the model. The fitting routine clearly broke down.\n",
    "\n",
    "Again, show without the binned model data (in this case it shows the emission line much more clearly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    plot_spectrum_with_continuum_models(energies, fit_results, adaptive=True, discrete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we obtain the confidence intervals on the fit with the emission line (but note that the emission line has a rather low N_line found by the fitting routine, which indicates that there is no emission line present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for model, result in fit_results.iteritems():\n",
    "        print_confidence_intervals_and_plot_residuals(model, result, find_emissionline=True, plot_residuals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final conclusion is that the fits were poor without the emission line, but slightly better with the exponentially cut Gaussian model including an emission line. However, the intensity of the emission line is so low that it could very well be disregarded altogether.\n",
    "\n",
    "The only possibility I see left is that if there is an emission line it is either in the tail of the distribution (which is nearly impossible to find), or perhaps in the low-energy part of the spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "- [Professor Denzil Dexter](http://www.teachertube.com/video/professor-denzil-dexter-81357) of the University of Southern California has proposed a dark matter candidate particle, the _Dextron_, which should result in a Gaussian emission line of width 1 GeV at an energy of 45.3 GeV. In case this line isn't detected, use your data to set a $3\\sigma$ upper limit on the predicted number of photons that this line could contain.\n",
    "\n",
    "Note that in all cases you should estimate $p$-values and significances for your hypothesis tests. You should explain your reasoning and choices and discuss how you interpret your results as you go along. Make sure that you make sensible decisions about how your report your results, e.g. about the number of significant figures used. You should also, in as much as possible, determine confidence intervals on the best-fitting model parameters (for continuum and emission line models). If errors appear to be correlated between parameters, you should also attempt to plot 2-dimensional confidence contours.Overall, we are looking for evidence that you understand and can apply the material in the course on model-fitting, parameter estimation and hypothesis testing, so you will find the information in the lectures in weeks 4, 5 and 6 to be very useful, as well as the case studies lecture from week 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To answer this question the fitting routine has to be performed again but setting the E_cent parameter fixed to 45.3 GeV (not leaving it as a free fit parameter), and assuming all other parameters equal to the best-fit parameters. Then we will have only N_line fluctuate and calculate the difference between the best-fit model and the new fit with the 'phantom'-emission line at 45.3 GeV. Once the $chi^2$ value is such that we can obtain a three sigma 'result', we will know how much photons should have been measured to speak about an actual three sigma result. However, I think this does not make sense at all since the best-fit parameters are very sketchy as we have very low $p$-values.\n",
    "\n",
    "In order to obtain the 3$\\sigma$ upper limit we steal Riley's code again (this time from the previous assignment question 6).\n",
    "\n",
    "Note that the number of photons can be obtained by taking the density value and multiplying it by the bin width.\n",
    "\n",
    "The best-fit model found is the expcut (without an emission line):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  Using scipy.optimize.minimze to minimize chi^2 yields:\n",
    "    [MLEs], chisq/dof: [ 0.10875274  1.58259172  0.73248181  1.92148886] 1.87935138488\n",
    "\n",
    "  Using scipy.optimize.curve_fit to obtain confidence intervals yields:\n",
    "    N_0 = 0.109 +/- 0.132\n",
    "    E_0 = 1.583 +/- 0.134\n",
    "    Gamma = 0.732 +/- 0.141\n",
    "    E_cut = 1.921 +/- 0.133\n",
    "    p-value for this fit = 0.00028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the corresponding sigma for given p-value - this of course assumes the incorrect distribution (normal)\n",
    "def p_to_sigmas(pval):\n",
    "    nd = scipy.stats.norm(0., 1.)\n",
    "    return nd.ppf(1.0-pval/2.)\n",
    "\n",
    "# We can then output the p-value with the corresponding significance, remembering to quote sensible precision\n",
    "# print \"The p-value is %.3f and this corresponds to a %.3f sigma significance\"%(p,p_to_sigmas(p))\n",
    "\n",
    "# First let's create a function to convert the sigma siginificance into a p-value\n",
    "def sigmas_to_p(sigval):\n",
    "    nd = scipy.stats.norm(0., 1.)\n",
    "    return 2.*(1.-nd.cdf(sigval))\n",
    "\n",
    "# This is the p-value required for a three sigma detection\n",
    "p = sigmas_to_p(3)\n",
    "print \"Required p-value for 3 sigma confidence:\", p\n",
    "\n",
    "# Create a function that we want to minimize - the difference between the known p value and \n",
    "# the p-value returned by the poisson distibrution with the total count rate (source+background)\n",
    "def f(lam):\n",
    "    return abs(p - scipy.stats.poisson.cdf(7,lam))\n",
    "\n",
    "def dextron(nline):\n",
    "    N_0 = 1.09\n",
    "    E_0 = 1.583\n",
    "    Gamma = 0.732\n",
    "    E_cut = 1.921\n",
    "    E_cent = 45.3/numpy.mean(parse_and_clean_dataset())\n",
    "    \n",
    "    ml_vals = [N_0, E_0, Gamma, E_cut, nline, E_cent]\n",
    "    dens, edges, err = bin_data_adaptive(energies)\n",
    "    \n",
    "    # NB if we would have found an emission line as best-fit model,\n",
    "    # we would have had to add an additional Dextron emission line\n",
    "    return stat(ml_vals, edges, dens, err, \"expcut_gauss\")\n",
    "\n",
    "def dextron_pvalue():\n",
    "    # Here we have to calculate the p-value of the obtained chisquared using the dextron fuction,\n",
    "    # which is a function of the number of degrees of freedom\n",
    "    # Next we have to return the absolute value of the p-value required for a three sigma detection\n",
    "    # minus the p-value for the Dextron-fit corrected for the best-fit. This we can optimize\n",
    "    # in order to find the p-value\n",
    "    \n",
    "    # This is the p-value required for a three sigma detection\n",
    "    p = sigmas_to_p(3)\n",
    "    # print \"Required p-value for 3 sigma confidence:\", p\n",
    "    \n",
    "    return abs(p - dextron_p)\n",
    "\n",
    "# Optimize the function above such that we find the lambda that returns a value as close to the given \n",
    "# p-value as possible.....start from lambda = 10 and let the optimiser find the true value\n",
    "# result = scipy.optimize.minimize(f,10,method='BFGS')\n",
    "# print \"The 3-sigma upper limit is:  \",result['x'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Well, this assignment is not yet finished..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
